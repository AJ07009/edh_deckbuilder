{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\python312\\lib\\site-packages (from scrapy) (24.7.0)\n",
      "Collecting cryptography>=36.0.0 (from scrapy)\n",
      "  Using cached cryptography-43.0.0-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\python312\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Using cached itemloaders-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Using cached parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Using cached pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\python312\\lib\\site-packages (from scrapy) (1.7.0)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Using cached service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\python312\\lib\\site-packages (from scrapy) (2.2.0)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\python312\\lib\\site-packages (from scrapy) (7.0.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\python312\\lib\\site-packages (from scrapy) (0.3.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\python312\\lib\\site-packages (from scrapy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from scrapy) (70.0.0)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from scrapy) (24.0)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Using cached tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\python312\\lib\\site-packages (from scrapy) (5.2.2)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\python312\\lib\\site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\python312\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->scrapy)\n",
      "  Using cached cffi-1.17.0-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\python312\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (24.8.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.12.1)\n",
      "Requirement already satisfied: idna in c:\\python312\\lib\\site-packages (from tldextract->scrapy) (3.7)\n",
      "Collecting requests>=2.1.0 (from tldextract->scrapy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Using cached requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\python312\\lib\\site-packages (from tldextract->scrapy) (3.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.1)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.1.0->tldextract->scrapy)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
      "Using cached cryptography-43.0.0-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Using cached itemloaders-1.3.1-py3-none-any.whl (12 kB)\n",
      "Using cached parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
      "Using cached pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
      "Using cached service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Using cached tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "Using cached cffi-1.17.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Installing collected packages: parsel, cffi, certifi, requests, itemloaders, cryptography, service-identity, requests-file, pyOpenSSL, tldextract, scrapy\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python312\\\\Scripts\\\\tldextract.exe' -> 'c:\\\\Python312\\\\Scripts\\\\tldextract.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crochet in c:\\python312\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: Twisted>=16.0 in c:\\python312\\lib\\site-packages (from crochet) (24.7.0)\n",
      "Requirement already satisfied: wrapt in c:\\python312\\lib\\site-packages (from crochet) (1.16.0)\n",
      "Requirement already satisfied: attrs>=21.3.0 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (24.2.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (24.8.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (24.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (4.12.1)\n",
      "Requirement already satisfied: zope-interface>=5 in c:\\python312\\lib\\site-packages (from Twisted>=16.0->crochet) (7.0.1)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\python312\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (3.7)\n",
      "Requirement already satisfied: setuptools>=61.0 in c:\\python312\\lib\\site-packages (from incremental>=24.7.0->Twisted>=16.0->crochet) (70.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy-selenium\n",
      "  Using cached scrapy_selenium-0.0.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting scrapy>=1.0.0 (from scrapy-selenium)\n",
      "  Using cached Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting selenium>=3.9.0 (from scrapy-selenium)\n",
      "  Using cached selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (24.7.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (43.0.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (1.3.1)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (1.9.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (24.2.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (2.2.0)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (7.0.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (0.3.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (70.0.0)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (24.0)\n",
      "Requirement already satisfied: tldextract in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (5.1.2)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (5.2.2)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\python312\\lib\\site-packages (from scrapy>=1.0.0->scrapy-selenium) (2.0.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.9.0->scrapy-selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\python312\\lib\\site-packages (from selenium>=3.9.0->scrapy-selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\python312\\lib\\site-packages (from selenium>=3.9.0->scrapy-selenium) (4.12.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\python312\\lib\\site-packages (from selenium>=3.9.0->scrapy-selenium) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python312\\lib\\site-packages (from cryptography>=36.0.0->scrapy>=1.0.0->scrapy-selenium) (1.17.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\python312\\lib\\site-packages (from itemloaders>=1.0.1->scrapy>=1.0.0->scrapy-selenium) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy>=1.0.0->scrapy-selenium) (24.2.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy>=1.0.0->scrapy-selenium) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\python312\\lib\\site-packages (from service-identity>=18.1.0->scrapy>=1.0.0->scrapy-selenium) (0.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium>=3.9.0->scrapy-selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium>=3.9.0->scrapy-selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy>=1.0.0->scrapy-selenium) (24.8.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy>=1.0.0->scrapy-selenium) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy>=1.0.0->scrapy-selenium) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in c:\\python312\\lib\\site-packages (from Twisted>=18.9.0->scrapy>=1.0.0->scrapy-selenium) (24.7.2)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\python312\\lib\\site-packages (from tldextract->scrapy>=1.0.0->scrapy-selenium) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\python312\\lib\\site-packages (from tldextract->scrapy>=1.0.0->scrapy-selenium) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\python312\\lib\\site-packages (from tldextract->scrapy>=1.0.0->scrapy-selenium) (3.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy>=1.0.0->scrapy-selenium) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy>=1.0.0->scrapy-selenium) (3.3.2)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.9.0->scrapy-selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached scrapy_selenium-0.0.7-py3-none-any.whl (6.7 kB)\n",
      "Using cached Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
      "Using cached selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
      "Using cached trio-0.26.2-py3-none-any.whl (475 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, pysocks, outcome, h11, wsproto, trio, trio-websocket, selenium, scrapy, scrapy-selenium\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python312\\\\Scripts\\\\scrapy.exe' -> 'c:\\\\Python312\\\\Scripts\\\\scrapy.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scrapy \n",
    "%pip install crochet \n",
    "%pip install scrapy-selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy_selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterable\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy_selenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeleniumRequest\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdataFormating\u001b[39;00m(scrapy\u001b[38;5;241m.\u001b[39mitem):\n\u001b[0;32m      7\u001b[0m     card_name \u001b[38;5;241m=\u001b[39m scrapy\u001b[38;5;241m.\u001b[39mField()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy_selenium'"
     ]
    }
   ],
   "source": [
    "import scrapy \n",
    "from typing import Iterable\n",
    "from scrapy_selenium import SeleniumRequest\n",
    "\n",
    "class dataFormating(scrapy.item):\n",
    "\n",
    "    card_name = scrapy.Field()\n",
    "\n",
    "class databaseSpider(scrapy.Spider):\n",
    "    name = \"databaseSpider\"\n",
    "\n",
    "    custom_settings = {\n",
    "        'FEED_FORMAT':'json',\n",
    "        'FEED_URI':f'databaseSpider.json',\n",
    "    }\n",
    "\n",
    "def start_requests(self) -> Iterable[scrapy.Request]:\n",
    "    base_url = 'https://gatherer.wizards.com/Pages/Search/Default.aspx?color=|[W]|[U]|[B]|[R]|[G]'\n",
    "\n",
    "    for url in base_url:\n",
    "        yield SeleniumRequest(url=url, callback=self.parse)\n",
    "    return super().start_requests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 12:45:31 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-08-19 12:45:31 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.0, Twisted 24.7.0, Python 3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.1 4 Jun 2024), cryptography 43.0.0, Platform Windows-10-10.0.19045-SP0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'databaseSpider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrawler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlerProcess\n\u001b[0;32m      3\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess({\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSER_AGENT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m })\n\u001b[1;32m----> 7\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(\u001b[43mdatabaseSpider\u001b[49m)\n\u001b[0;32m      8\u001b[0m process\u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'databaseSpider' is not defined"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "    \n",
    "process.crawl(databaseSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
